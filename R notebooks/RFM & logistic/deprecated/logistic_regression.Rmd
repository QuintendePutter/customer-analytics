---
title: "Logistic Regression"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---
### Introduction

Logistic regression is the most commonly used statistical technique for binary response data. Many marketing applications are concerning binary consumer decisions: 

* does a consumer respond or not respond to marketing? 
* do they subscribe or not subscribe?
* do they churn or not churn?

We'll use a data set on customer churn for a telecommunications company with several different services. We'll use demographic, service usage, and customer history to predict churn. We then apply this model to a new, holdout set of customers.  We calclate the confusion matrix, the lift table, and use it to do targeted proactive churn selection.

### Installing the packages and loading the data

```{r, warning=FALSE, message=FALSE, error=FALSE}
library(foreign)
library(janitor)
library(tidyverse)
library(car)
library(pROC)


options("scipen"=200, "digits"=3)

# set working directory 
# setwd("C:/Users/gknox/Dropbox/Customer Analytics/R notebooks/RFM & logistic")

# telco<-read.csv('./data/telco.csv')

telco<-read.csv(file.choose())

# drop the ID column, make senior citizen a factor variable, and divide totalcharges by 1000
telco<-telco[-c(1)]
telco$SeniorCitizen<-as.factor(telco$SeniorCitizen)
telco$TotalCharges<-telco$TotalCharges/1000
```

### Inspecting the data

Every row is a customer. We have data on demographics, the type of services they own, payment methods, and the dependent variable, whether they churn. 
```{r}
head(telco)
```

You can see the summary statistics of the variables.  Tenure, Monthly Charges and Total Charges are the only continuous variables; all the others, including churn, are categorical. 
```{r}
summary(telco)
```

### Churn

What fraction of customers churn (quit)?  This is the dependent variable we want to predict.  We need to use the "as.numeric" function to transform it from a factor variable to a 0/1 continuous variable in R.  We report the average churn rate of the customer below. 
```{r}
Churn.num<-as.numeric(telco$Churn)-1
rbar<-mean(Churn.num)
```

The average churn rate in the customer base is `r round(rbar,3)`.

### Tenure

One important driver of churn is likely to be **tenure**, how long a customer has been a customer for. 
We can see below that there is a spike at 1, many customers just started, and a smaller peak at 72.
```{r}

hist(telco$tenure, main = "", xlab="Tenure (# months a customer)", breaks = 71)
```

How does the rate churn vary by tenure?

```{r}
churn_tenure<-aggregate(Churn.num,list(telco$tenure), mean)
bp<-barplot(churn_tenure[,2]~churn_tenure[,1], main="Churn probability decreases with tenure", xlab="Tenure (# months a customer)", ylab="P(churn | tenure)",xaxt="n")
axis(1, at = bp[,1], labels=churn_tenure[,1], cex.axis=0.8, las=1)
```

The figure shows a clear negative relationship: the longer the customer has been a customer, the lower the probability of churn (churn rate).  

### Contract Type

Another variable likely to be important is the type of contract the customer is on.  There are three types, with this distribution of occurring in the data:

```{r}
table(telco$Contract)
churn_contract<-aggregate(Churn.num, list(telco$Contract), mean)
barplot(churn_contract[,2]~churn_contract[,1], main="", xlab="Type of contract", ylab="P(Churn | contract type)")
```

Clearly, month-to-month contract customers are more likely to churn than customers locked into one or two year contracts.  

### Estimating the logistic regression

So far this analysis has only looked at the effect of *one variable at a time* on churn. To investigate how many variables together affect churn, we need to use a model. 

For each customer $i = 1, 2, \dots I$, the probability of churn is a function of the variables $X_i$ for that customer and the coefficients $\beta$ to be estimated.

$$
P(\textrm{Churn}_i = 1) = \frac{e^{X_{i}'\beta}}{1+e^{X_{i}'\beta}}
$$
where $X_i' \beta = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \dots \beta_P X_{Pi}$ is the sum of the product of the variables (including the intercept) and the coefficients. The models differ in what is included as in $X$.  They vary in terms of how many variables are included. 

* **Model 1** is the simplest: every variable is included; tenure is treated as a continuous variable 
$$
X_i' \beta = \beta_1 + \beta_2 1\{\textrm{gender}_i = \textrm{male}\} + \beta_2 1\{\textrm{SeniorCitizen}_i=1\} + \dots + \beta_{24} \textrm{tenure}_i
$$

```{r}
options(width = 200)
model_1 <- glm(Churn ~ gender+SeniorCitizen+Partner+Dependents+PhoneService
             +MultipleLines+InternetService+OnlineSecurity+OnlineBackup+
               DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+
               PaperlessBilling+PaymentMethod+MonthlyCharges+
               TotalCharges+tenure
             ,data=telco,family="binomial")

# another way of writing this is "~ ." which means regress Churn on everything else in the data set.

model_1 <- glm(Churn ~ . , data=telco, family="binomial")
summary(model_1)
# stargazer(model_1, type = "text")
```

* Model 1 has `r length(coef(model_1))` coefficients.

* **Model 2** is more complex: like Model 1, except that tenure is treated a categorical variable. In other words there is a dummy variable for every level of tenure but one. $\beta_{1} 1\{\textrm{tenure}_i=1\} + \beta_{2} 1\{\textrm{tenure}_i=2\} + \dots \beta_{71} 1\{\textrm{tenure}_i=71\}$. This way, we can flexibly capture a pattern between tenure and churn. In R, all you have to do is write **as.factor(tenure)** instead of **tenure**. 

```{r}
model_2 <- glm(Churn ~ gender+SeniorCitizen+Partner+Dependents+PhoneService
             +MultipleLines+InternetService+OnlineSecurity+OnlineBackup+
              DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+
              PaperlessBilling+PaymentMethod+MonthlyCharges+
             TotalCharges+as.factor(tenure)
             ,data=telco,family="binomial")

# another way to subtract tenure from everything else and add as.factor(tenure) back: except var1 . - var1

model_2 <- glm(Churn ~ . +as.factor(tenure) -tenure , data=telco, family="binomial")
summary(model_2, digits=3)
```

* Model 2 has `r length(coef(model_2))` coefficients.

* **Model 3** is the most complex: like Model 2, except that there is an interaction between payment type and tenure.  Note in general and interaction is the coefficient on the product of two variables.

$$
  = \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_2 x_1 + \dots
$$
Here we have an interaction between all tenure levels (except one) and all payment levels (except one).  For example, here is one type:

$$
= \beta_{1} 1\{\textrm{tenure}_i=1\} + \beta_{2} 1\{\textrm{payment_type}_i=\textrm{credit}\} + \beta_{3} 1\{\textrm{payment_type}_i=\textrm{credit}\}  1\{\textrm{tenure}_i=1\}  + \dots.
$$

```{r}
# model 3 is too tedious to write out the long way. the short way is to understand that var1*var2 = var1 + var2 + var1 var2. So we remove tenure and payment method and add them with the star in betwen. 
model_3 <- glm(Churn ~ . +as.factor(tenure)*as.factor(PaymentMethod) -tenure -PaymentMethod, data=telco, family="binomial")
summary(model_3, digits=3)

```

* Model 3 has `r length(coef(model_3))` coefficients.

* So, we've estimated 3 models each one increasing in the number of coefficients.  Let's see how well they predict.

### Deviance and proportion of deviance explained (R2)

Deviance is an error measure, $-2 \ln(\textrm{likelihood})$. We want it to be as small as possible. If we had a model where there were as many parameters as observations, a fully saturated model, it would be zero.

The residual deviance is the deviance associated with the full model.
The null deviance is the deviance for a model where there is only an intercept, which is the same as saying that every customer has the same probability of churning, equal to `r rbar`.
The difference between the residual and the null deviance then gives us some sense of how well our model fits overall, taken together. You can also look at the proportion of deviance explained by the variables in the model.

$$
R^2 = \frac{D_0 - D}{D_0} = 1 - \frac{D}{D_0}
$$
```{r}

D<-model_1$deviance
D0<-model_1$null.deviance

R21<-1-D/D0

D<-model_2$deviance
D0<-model_2$null.deviance

R22<-1-D/D0

D<-model_3$deviance
D0<-model_3$null.deviance

R23<-1-D/D0

IS=c(R21,R22,R23)

barplot(IS, names.arg = c("model 1", "model 2", "model 3"), main=expression(paste("In-Sample R"^"2")), xlab="Model", ylab=expression(paste("R"^"2")))
```

Models 1, 2 and 3 are explaining `r round(R21,2)*100`%, `r round(R22,2)*100`% and `r round(R23,2)*100`%, respectively, of the deviance in customer churn.

### Overfitting, K-fold out of sample

**But, is the better performance of model a result of overfitting?**

What we really care about is being able to predict **new** data.  The R2 and deviance measures are all about in-sample, not out-of-sample fit. So it doesn't tell us how well our model performs on other data.

We can mimic the presence of new data by holding out part of the data. 

We use K-fold out of sample validation. Given a dataset of $n$ observations, $\{X_i, Y_i\}_{i=1}^{n}$

* Split the data into $K$ evenly sized random subsets (folds).
* For $k=1 \dots K$
  - Fit the coefficients, $\hat{\beta}$, using all except the $k^{\textrm{th}}$ fold of the data.
  - Record the $R^{2}$ on the fold held out.
Then we have $K$ out-of-sample $R^2$ measures. These are an estimate of the distribution of the model's predictive performance.

```{r}
set.seed(19103)
n = nrow(telco)
K = 10
foldid = rep(1:K, each=ceiling(n/K))[sample(1:n)]
# foldid[1:10]
OOS <- data.frame(model1=rep(NA,K), model2=rep(NA,K), model3=rep(NA,K))


## pred must be probabilities (0<pred<1) for binomial
  deviance <- function(y, pred, family=c("gaussian","binomial")){
    family <- match.arg(family)
    if(family=="gaussian"){
      return( sum( (y-pred)^2 ) )
    }else{
      if(is.factor(y)) y <- as.numeric(y)>1
      return( -2*sum( y*log(pred) + (1-y)*log(1-pred) ) )
    }
  }

## get null devaince too, and return R2

  
  R2 <- function(y, pred, family=c("gaussian","binomial")){
  fam <- match.arg(family)
  if(fam=="binomial"){
    if(is.factor(y)){ y <- as.numeric(y)>1 }
  }
  dev <- deviance(y, pred, family=fam)
  dev0 <- deviance(y, mean(y), family=fam)
  return(1-dev/dev0)
}  

for(k in 1:K){
  train = which(foldid!=k) # data used to train
  
  # fit regressions
  model_1 <- glm(Churn ~ . , data=telco[train,], family="binomial")
  summary(model_1)
  
  model_2 <- glm(Churn ~ . +as.factor(tenure) -tenure, data=telco[train,], family="binomial")
  summary(model_2)
  
  model_3 <- glm(Churn ~ . +as.factor(tenure)*as.factor(PaymentMethod) -tenure -PaymentMethod, data=telco[train,], family="binomial")
  summary(model_3)
  
  
  # predict on holdout data (-train)
  pred1<- predict(model_1, newdata=telco[-train,], type = "response")
  pred2<- predict(model_2, newdata=telco[-train,], type = "response")
  pred3<- predict(model_3, newdata=telco[-train,], type = "response")
  
  # calculate R2
  OOS$model1[k]<-R2(y = telco$Churn[-train],pred=pred1, family="binomial")
  OOS$model2[k]<-R2(y = telco$Churn[-train],pred=pred2, family="binomial")
  OOS$model3[k]<-R2(y = telco$Churn[-train],pred=pred3, family="binomial")
  
  # print progress
  cat(k, "  ")
    
}
boxplot(OOS[,1:3], data=OOS, main=expression(paste("Out-of-Sample R"^"2")),
        xlab="Model", ylab=expression(paste("R"^"2")))
```


* Model 3 had the highest in-sample $R^2$, and now it has the worst out-of-sample $R^2$. It's even **negative**!  In other words, it is worse that random guessing.  It would be better to guess that every customer has a `r rbar` probability of churning rather than using Model 3 to figure out which customers are more likely to churn than others.

* Bottom line: Model 3 is over-fitting. It is capturing patterns in the in-sample data that do not generalize to the out-of-sample data. This is why it does such a poor job at predicting.

* Models 1 and 2 have basically the same out of sample $R^2$. 

* This means favoring the simpler models.  Model 1, being the simplest, and tied for the best predictive performance is the winner.  

* Note the simplest model here won, but that isn't always the case. It depends on the data. The best model might be the "middle" or even the most complex model. You have to check out-of-sample performance.


### Interpreting Coefficients

We continue with model 1.  Face validity check: do the coefficients make sense?

* Given the figures earlier, we would expect to see a *negative* coefficients on tenure and length of contract. Do we see this?

```{r}
summary(model_1)
```

* How do we interpret the quantitative effect of the coefficients?  e.g., how much more or less likely is a senior citizen to churn?
```{r}
cat('coefficient:', coef(model_1)["SeniorCitizen1"],"\n")

cat('multiplicative effect on odds, exp(coefficient):', exp(coef(model_1)["SeniorCitizen1"]),"\n")
ocoef<-round(exp(coef(model_1)["SeniorCitizen"]),2)

cat('percent change in odds: exp(coefficient)-1:', exp(coef(model_1)["SeniorCitizen1"])-1,"\n")
ecoef<-round(exp(coef(model_1)["SeniorCitizen1"])-1,2)*100
```

So the odds of churning are multiplied by `r ocoef` when it's a senior citizen.  Another way of saying that is that senior citizens have `r ecoef`% higher odds of churning.

* What is the effect of tenure (# months as customers) on churn?
```{r}
cat('coefficient:', coef(model_1)["tenure"],"\n")

cat('multiplicative effect on odds, exp(coefficient):', exp(coef(model_1)["tenure"]),"\n")

cat('percent change in odds: exp(coefficient)-1:', exp(coef(model_1)["tenure"])-1,"\n")

cat('percent change in odds after 12 months: exp(12*coefficient)-1:', exp(12*coef(model_1)["tenure"])-1,"\n")
```

Note there is the significance of an individual predictor (e.g., ContractOne year) and the joint significance of a group of coefficients (e.g., all contract terms):

```{r}
linearHypothesis(model_1, c("ContractOne year = 0", "ContractTwo year = 0"))
```

Here we test jointly that the two estimated contract coefficients are zero.  We reject the null hypothesis.

### predicting

Here we use model 1 to predict the probability of default for a certain customer with a specific profile: a male, senior citizen without a partner or dependents, etc. See below.

```{r}

newdata = data.frame(gender = "Male", SeniorCitizen=as.factor(1),Partner="No",Dependents="No", tenure=72,PhoneService="Yes",MultipleLines="No", InternetService="DSL", OnlineSecurity="No", OnlineBackup="No", DeviceProtection="No", TechSupport="Yes", StreamingTV="Yes", StreamingMovies="No", Contract="One year", PaperlessBilling="No", PaymentMethod="Mailed check", MonthlyCharges=30,TotalCharges=1)

predict(model_1,newdata,type="response")

```

The probability he defaults given Model 1 is low.

### Holdout sample

Now we look at how well model 1 performs on one holdout sample, **holdout_telco.csv**. 

```{r}
holdout_telco<-read.csv('./data/holdout_telco.csv')

# change senior citizen to factor variable, divide total charges by 1000 just like telco
holdout_telco$SeniorCitizen<-as.factor(holdout_telco$SeniorCitizen)
holdout_telco$TotalCharges<-holdout_telco$TotalCharges/1000


# Ensure labels in holdout are the same as labels in telco
levels(holdout_telco$InternetService)[2]<-"Fiber optic"
levels(holdout_telco$Contract)<-levels(telco$Contract)
levels(holdout_telco$PaymentMethod)<-levels(telco$PaymentMethod)

# calculate average churn in holdout
holdout_telco$Churn.num<-as.numeric(holdout_telco$Churn)-1

n_churners<-sum(holdout_telco$Churn.num)

rbar_ho<-mean(holdout_telco$Churn.num)
```

The churn rate we see in the holdout sample, `r round(rbar_ho,3)`, is close to that in the estimation sample we used earlier, `r rbar`.

Now we use the model estimated on the other data to make predictions on this new data. Note that our predicted probabilities lie between 0 and 1, whereas our data are binary. We can get the predictions for each customer and graph them with the 0/1 churn decisions.

```{r}
# predicted x'beta part of 
xb <- predict(model_1, type = "link", newdata=holdout_telco)
# the predicted probability 
prob <- predict(model_1, type = "response", newdata=holdout_telco)

head(cbind(xb,prob))

# order customers from least likely to churn (according to model) to most likely
ind<-order(prob)
head(prob[ind])



plot(xb[ind],holdout_telco$Churn.num[ind], pch=4,cex=0.3,col="blue", xlab="x'beta",ylab="P(Churn) on holdout data")
points(xb[ind],prob[ind])
legend('left',legend=c("actual", "predicted (model 1)"),col=c("blue","black"), pch=c(4,1))
```

### Confusion matrix

We can also *classify* predictions by turning them into 0's and 1's. If $\hat{p}_i > 0.5, \; \textrm{pred} = 1$ otherwise 0.  You can use other cutoffs if the cost of making one type of error (e.g., predicting 0 when the truth is 1) is different than the other, or if the overal average proportion of the dependent variable is close to 0 or 1.  

After converting our probabilities to binary predictions, we can compare our predicted 0's and 1's with the actual 0's and 1's. From this we can get quantities such as the accuracy, the hit rate (sensitivity), and the false positive rate.

```{r}
confusion_matrix <- (table(holdout_telco$Churn, prob > 0.5))
confusion_matrix <- as.data.frame.matrix(confusion_matrix)
colnames(confusion_matrix) <- c("No", "Yes")
confusion_matrix$Percentage_Correct <- confusion_matrix[1,]$No/(confusion_matrix[1,]$No+confusion_matrix[1,]$Yes)*100
confusion_matrix[2,]$Percentage_Correct <- confusion_matrix[2,]$Yes/(confusion_matrix[2,]$No+confusion_matrix[2,]$Yes)*100
print(confusion_matrix)
cat('Overall Percentage:', (confusion_matrix[1,1]+confusion_matrix[2,2])/nrow(holdout_telco)*100)
```

### ROC curves

```{r}

par(mai=c(.9,.8,.2,.2))
plot(roc(holdout_telco$Churn.num, prob), print.auc=TRUE,
     col="black", lwd=1, main="ROC curve", xlab="Specificity: true negative rate", ylab="Sensitivity: true positive rate", xlim=c(1,0))

text(confusion_matrix$Percentage_Correct[[1]]/100, confusion_matrix$Percentage_Correct[[2]]/100, ".5 threshold")
abline(h=confusion_matrix$Percentage_Correct[[2]]/100, col="red",lwd=.3)
abline(v=confusion_matrix$Percentage_Correct[[1]]/100, col="red",lwd=.3)



```
### Lift curves

Lift is a common measure in marketing of model performance. The lift asks how much more likely are customers in the top $k^{\textrm{th}}$ decile to churn compared to the average.

The lift is usually defined by decile. You sort the customer base according to the predicted probability of churn, from largest to smallest, so the 1st decile is those who are most predicted to churn according to the model.  The lift of decile $k$ is defined as:

$$
\lambda_k = \frac{r_k}{\bar{r}}
$$
where $\lambda_k$ is the lift of decile $k$, $r_k$ is the churn rate for the $k$ decile, and $\bar{r}$ is the churn rate for the entire customer base.  $\lambda_k$ is how much more likely customers in decile $k$ are to churn compared to the churn rate for the entire sample.  

```{r}
ntiles <- function(x, bins) {
  quantiles = seq(from=0, to = 1, length.out=bins+1)
  cut(ecdf(x)(x),breaks=quantiles, labels=F)
}
# create deciles
prob_decile = ntiles(prob, 10)

# prob, decile and actual
tbl<-data.frame(cbind(prob,prob_decile, holdout_telco$Churn.num))
colnames(tbl)<-c("predicted","decile", "actual")

# create lift table by decile
# average churn rate by decile
lift<-aggregate(actual~decile, data = tbl, mean)
colnames(lift)[2]<-"actual churn rate"

# lift is the actual churn rate in the decile divided by average overall churn rate
lift[,3]<-lift[,2]/rbar_ho
colnames(lift)[3]<-"lift"

# order for highest to lowest
lift<-lift[order(-lift$decile),]

lift[,4]<-cumsum(lift$actual)/sum(lift$actual)*100
colnames(lift)[4]<-"cumulative lift"

lift
```

Customers in the top decile are the top 10% most likely to churn *according to our model*. The top decile lift is `r lift[[1,3]]`. Customers in the top decile are `r lift[[1,3]]` times more likely to *actually* churn than the average customer. 

The rightmost column shows the cumulative lift. The cumulative lift for the $k$ decile is the percentage of all churners accounted for cumulatively by the first $k$ deciles.  The first decile contains `r round(lift[[1,4]],0)`% of all churners in the data set (in total there are `r n_churners` churners in the holdout dataset). 

The cumulative lift of decile 2 is `r round(lift[[2,4]],0)`% of all churners are in the top 2 deciles. In the bottom most deciles there are barely any churners, so the cumulative lift increases little or not at all.

We can graph this out below. The top three deciles account for `r round(lift[[3,4]],0)`% of all churners.  We can use this to compare models.  The higher the lift for a given decile, the better the model.  A straight line, where we randomly sorted customers instead of using a model, is the naive model.

```{r echo=FALSE}
# order from highest to smallest in terms of prob
tbl <- tbl[order(tbl$predicted,decreasing = TRUE),]

# percentage of churners from beginning to end.
tbl$prop_churn <- cumsum(tbl$actual)/sum(tbl$actual)*100

# percentage of customers from beginning to end
tbl$prop_cust <- (seq(nrow(tbl))/nrow(tbl))*100

# Plotting percentage of churners as a function of percentage of customers

plot(tbl$prop_cust,tbl$prop_churn,type="l",xlab="% of customers targeted using model",ylab="% of churners accounted for",xlim = c(0,100), ,ylim = c(0,100), panel.first=grid(),col="blue")
legend(1, 95, legend=c("Naive", "Logistic"), col=c("red", "blue"), lty=1:1, cex=0.8)
abline(a=0,b=1,col="red")
text(x = 30,y= lift[[3,4]], paste("top 30%", round(lift[[3,4]],0) ))



```

* This gives us equivalent information to the churn table.

* targeting the top 10% using the model would give us `r tbl$prop_churn[which.min(abs(tbl$prop_cust-10))]`% of total churners in the data.

### Selecting deciles to target

Once we have used the model to put customers in the right decile, targeting is simple.
We calculate the profit from each n-tile and target customers who are in the profitable tiles. 
We will use the proactive churn framework from Blattberg, Kim and Neslin to calculate expected profits.  This approach takes into account the actual proportion of churners as identified by the model.


![Source: Database Marketing, Blattberg, Kim and Neslin 2008](./figures/proactive_churn.jpg)

The key parameter is $\beta_K$, the proportion of churners in the top $K$ deciles contacted.  
$$
\beta_K = \frac{\sum_{k=1}^{K} \; r_k \, n_k}{\sum_{k=1}^{K} \; n_k} \quad \textrm{where} \; K = 1, 2, .. \dots,  10
$$
where $n_k$ is the number of customers in decile $k$.  Let's say we want to target the top $K=2$ deciles; $r_1 = .1$ and $r_2 = .05$, and $n_1 = n_2 = 100$. Then $\beta_2 = \frac{.1*100 + .05*100}{200} = .075$.

Note that if $K=10$, i.e., everyone is targeted, $\beta_K = \bar{r}$, the proportion of churners in the dataset.

If he or she is churner, with some probability $\gamma$ he or she takes the offer, and is persuaded to stay rather than churn.  Thus the gain to the firm is that person's lifetime value ($LTV$), the discounted expected future profit to the firm when he or she stays, minus the cost of the offer $\delta$ and the cost of contacting the customer $c$.  If he or she is a non-churner, with some probability they take the incentive $\psi$ and their delight makes their lifevalue increase by some factor $\Delta$. Here we assume $\psi=1$, everyone takes the offer, and there is no added demand from customers delighted by being targeted $\Delta=0$. 

The better the model is at sorting customers into deciles, the higher $\beta$ will be, and the more well targeted will be the incentives.  Profits are: 

*  increasing the more effective the incentive, the higher $\gamma$
*  increasing in the lifetime value ($LTV$) of a rescued customer
*  decreasing in the cost of the incentive ($\delta$) and the cost of contacting the targeted customers ($c$).

We calculate $\beta$, the probability that a targeted customer is a churner, by taking the cumulative proportion of churners in the top $k$ deciles.

```{r}

gamma = 0.1  # probability that customer is rescued if he or she is a churner
LTV = 500   # lifetime value of rescued customer
delta = 50  # cost of incentive
c = 0.50  # cost of contact

# re-order lift from highest to lowest
lift<-lift[order(-lift$decile),]

# add columns to our lift table

count<-aggregate(actual ~ decile, data = tbl, FUN = length)
lift$n_customers<-count[order(-count$decile),2]

lift$cum_customers<-cumsum(lift$n_customers)

lift$cum_prop_churners<-cumsum(lift$`actual churn rate`*lift$n_customers)/lift$cum_customers

lift$profit <- lift$cum_customers*((gamma*LTV+delta*(1-gamma))*lift$cum_prop_churners-delta-c)

# renumber deciles for graph so that 1 is highest, not 10.
lift$decile<-11-lift$decile

lift

par(mai=c(.9,.8,.2,.2))
bp<-barplot(lift$profit ~ lift$decile, main="Optimal # of deciles to target", xlab="# deciles targeted", ylab="expected profits")
```



We see from the table below that given this model, the profit maximizing number of deciles to target is the top 2.  

